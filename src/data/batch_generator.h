#pragma once

#include "3rd_party/threadpool.h"
#include "common/options.h"
#include "data/batch_stats.h"
#include "data/iterator_facade.h"
#include "data/rng_engine.h"
#include "training/training_state.h"

#include "graph/expression_graph.h"
#include "data/revo_stub.h"
#include "data/competence.h"


#include <marian.h>
#include <condition_variable>
#include <deque>
#include <functional>
#include <mutex>
#include <queue>

namespace marian {
namespace data {

// Iterator over batches generated by a BatchGenerator. Mean to be the only
// interface to create batches.
template <class BatchGenerator>
class BatchIterator
    : public IteratorFacade<BatchIterator<BatchGenerator>, typename BatchGenerator::BatchPtr> {
private:
  BatchGenerator* bg_;
  typename BatchGenerator::BatchPtr current_;

  friend BatchGenerator;

  // private, only BatchGenerator is allowed to create pointers
  // hence friend class above.
  BatchIterator(BatchGenerator* bg, typename BatchGenerator::BatchPtr current)
      : bg_(bg), current_(current) {}

public:
  virtual bool equal(const BatchIterator& other) const override {
    // iterators are only equal if they point at the same batch or both have nullptr
    return current_ == other.current_;
  }

  // Just returns the batch pointer
  virtual const typename BatchGenerator::BatchPtr& dereference() const override { return current_; }

  // sets current pointer to the next batch pointer, will be nullptr when no
  // batches are available. This will evaluate to false in a for loop.
  virtual void increment() override { current_ = bg_->next(); };
};
template <class DataSet>
class BatchGenerator : public RNGEngine {
public:
  typedef typename DataSet::batch_ptr BatchPtr;

  typedef typename DataSet::Sample Sample;
  typedef std::vector<Sample> Samples;  // @TODO: type names should be capitalized

  typedef BatchIterator<BatchGenerator> iterator;
  friend iterator;

protected:
  Ptr<DataSet> data_;
  std::vector<Ptr<DataSet>> data_set_;
  Ptr<Options> options_;
  bool restored_{false};
  bool shuffle_;

private:
  // CL
  unsigned int compentence_runner_ = 1;
  unsigned int total_batches_runner = 1;
  unsigned int skipped_ = 0;
  Ptr<Competence> competence_;
  float avg_mod_ = 10;
  bool first_CL_completed = true;
  //
  std::mutex mutex_;
  // Gap training
  size_t epoch_ = 1;
  Ptr<DataTrainingBase> gap_training_;
  bool IsAsync_ = true;

  Ptr<BatchStats> stats_;

  // state of fetching
  std::deque<BatchPtr> bufferedBatches_;  // current swath of batches that next() reads from

  // state of reading
  typename DataSet::iterator current_;
  bool newlyPrepared_{true};  // prepare() was just called: we need to reset current_  --@TODO: can
                              // we just reset it directly?

  // variables for multi-threaded pre-fetching
  mutable ThreadPool threadPool_;  // (we only use one thread, but keep it around)
  std::future<std::deque<BatchPtr>>
      futureBufferedBatches_;  // next swath of batches is returned via this
  std::future<std::deque<BatchPtr>> futureBufferedBatches2_;

  // this runs on a bg thread; sequencing is handled by caller, but locking is done in here
  std::deque<BatchPtr> fetchBatches_CL2(size_t id) {
    // CL training
    float current_c = 1;
    if (options_->get<bool>("print_mod") && id == 0)
        avg_mod_ = gap_training_->get_mod_from_emb(compentence_runner_);
    if(competence_ && competence_->schedule == "mod") {
      current_c = competence_->cal_compentence(compentence_runner_);
      // Mod training
      if(compentence_runner_ != 1) {
        if(id == 0){
          avg_mod_ = gap_training_->get_mod_from_emb(compentence_runner_);
          if ((bool)gap_training_->get_init_value())
            competence_->set_T(gap_training_->get_init_value());
        }
        current_c = competence_->cal_compentence(avg_mod_);
      }
    } else if(competence_)
      competence_->cal_compentence(compentence_runner_);

    // LOG(info, "fillBatches entered {}", id);
    typedef typename Sample::value_type Item;
    auto itemCmp = [](const Item& sa, const Item& sb) {
      return sa.size() < sb.size();
    };  // sort by element length, not content

    auto cmpSrc = [itemCmp](const Sample& a, const Sample& b) {
      return std::lexicographical_compare(a.begin(), a.end(), b.begin(), b.end(), itemCmp);
    };

    auto cmpTrg = [itemCmp](const Sample& a, const Sample& b) {
      return std::lexicographical_compare(a.rbegin(), a.rend(), b.rbegin(), b.rend(), itemCmp);
    };

    auto cmpNone = [](const Sample& a, const Sample& b) {
      return &a < &b;
    };  // instead sort by address, so we have something to work with

    typedef std::function<bool(const Sample&, const Sample&)> cmp_type;
    typedef std::priority_queue<Sample, Samples, cmp_type> sample_queue;

    std::unique_ptr<sample_queue> maxiBatch;  // priority queue, shortest first

    if(options_->has("maxi-batch-sort")) {
      if(options_->get<std::string>("maxi-batch-sort") == "src")
        maxiBatch.reset(new sample_queue(cmpSrc));
      else if(options_->get<std::string>("maxi-batch-sort") == "none")
        maxiBatch.reset(new sample_queue(cmpNone));
      else
        maxiBatch.reset(new sample_queue(cmpTrg));
    } else {
      maxiBatch.reset(new sample_queue(cmpNone));
    }

    size_t maxBatchSize = options_->get<int>("mini-batch");
    size_t maxSize = maxBatchSize * options_->get<int>("maxi-batch");

    // try
    if (!options_->has("inference") && competence_ && competence_->schedule == "mod" && current_c != 1.) {
      if ((bool)gap_training_->get_init_value())
        maxSize = (int) (maxSize / (current_c * 2));
      else
        maxSize = (int) (maxSize / (current_c * 4));

      LOG(info, "[CL] maxsize: {}", maxSize);
    }
    // LOG(info, "Preloading batches");

    // consume data from corpus into maxi-batch (single sentences)
    // sorted into specified order (due to queue)
    std::unique_lock<std::mutex> Lock(mutex_);
    if(newlyPrepared_) {
      current_ = data_->begin();
      newlyPrepared_ = false;
    } else {
      if(current_ != data_->end())
        ++current_;
    }
    size_t sets = 0;
    while(current_ != data_->end() && maxiBatch->size() < maxSize) {  // loop over data
      maxiBatch->push(*current_);
      sets = current_->size();
      // do not consume more than required for the maxi batch as this causes
      // that line-by-line translation is delayed by one sentence
      bool last = maxiBatch->size() == maxSize;
      if(!last) {
        // std::lock_guard<std::mutex> statsLock(mutex_);
        // prevent other process reach the end line
        ++current_;  // this actually reads the next line and pre-processes it
      }
    }
    Lock.unlock();
    // size_t numSentencesRead = maxiBatch->size();

    // LOG(info, "Turning samples into batches");

    // construct the actual batches and place them in the queue
    Samples batchVector;
    size_t currentWords = 0;
    std::vector<size_t> lengths(sets, 0);  // records maximum length observed within current batch

    std::deque<BatchPtr> tempBatches;

    // process all loaded sentences in order of increasing length
    // @TODO: we could just use a vector and do a sort() here; would make the cost more explicit
    // LOG(info, "begin form batches, #lines = {}", maxiBatch->size());
    const size_t mbWords = options_->get<size_t>("mini-batch-words", 0);
    const bool useDynamicBatching = options_->has("mini-batch-fit");
    BatchStats::const_iterator cachedStatsIter;
    if(stats_)
      cachedStatsIter = stats_->begin();
    while(!maxiBatch->empty()) {  // while there are sentences in the queue
      // competence checking, not in validating
      SentenceTuple t_sam = maxiBatch->top();
      if (competence_) {
        if (competence_->schedule != "mod")
          current_c = competence_->cal_compentence(compentence_runner_);
      }
      if(!options_->has("inference") && competence_ && current_c != 1) {
        Words src_sent = t_sam[0];
        float complexity = competence_->cal_sentence_rarity(src_sent);
        if(complexity > current_c) {
          skipped_++;
          maxiBatch->pop();
          continue;
        }

        if (competence_->IsDW){
          float w = std::pow(complexity / current_c, competence_->d_ratio);
          t_sam.setWeights({w});
          // ??
          //w = std::max(float(0.5), w);
        }
      }
      //if(!options_->has("inference")){
      //  // set a weight for each sent base on complexity, test outside
      //  float complexity = competence_->cal_sentence_rarity(t_sam[0]);
      //  t_sam.setWeights({complexity / current_c});
      //}
      // push item onto batch
      batchVector.push_back(t_sam);
      maxiBatch->pop();  // fetch next-shortest

      // have we reached sufficient amount of data to form a batch?
      bool makeBatch;
      if(useDynamicBatching && stats_) {  // batch size based on dynamic batching
        for(size_t i = 0; i < sets; ++i)
          if(batchVector.back()[i].size() > lengths[i])
            lengths[i] = batchVector.back()[i].size();  // record max lengths so far

        maxBatchSize = stats_->findBatchSize(lengths, cachedStatsIter);
// LOG(info, "[data] refreshed maxbatchsize {}", maxBatchSize);
// this optimization makes no difference indeed
#if 1  // sanity check: would we find the same entry if searching from the start?
        auto it = stats_->lower_bound(lengths);
        auto maxBatchSize1 = stats_->findBatchSize(lengths, it);
        ABORT_IF(maxBatchSize != maxBatchSize1, "findBatchSize iter caching logic is borked");
#endif
        makeBatch = batchVector.size() >= maxBatchSize;
        // if last added sentence caused a bump then we likely have bad padding, so rather move it
        // into the next batch
        if(batchVector.size() > maxBatchSize) {
          maxiBatch->push(batchVector.back());
          batchVector.pop_back();
        }
      } else if(mbWords > 0) {
        currentWords += batchVector.back()[0].size();  // count words based on first stream =source
        // --@TODO: shouldn't we count based on
        // labels?
        // sum source + target
        currentWords += batchVector.back()[1].size();

        makeBatch = currentWords > mbWords;  // Batch size based on sentences
      } else
        makeBatch = batchVector.size() == maxBatchSize;  // Batch size based on words

      // if we reached the desired batch size then create a real batch
      if(makeBatch) {
        // filtering and increase competence_runner
        BatchPtr batch = data_->toBatch(batchVector);
        // skip failed batches, size * front_width + size * back_width
        size_t max_words = batch->size() * batch->front()->batchWidth()
                           + batch->size() * batch->back()->batchWidth();
        size_t batch_words = batch->front()->batchWords() + batch->back()->batchWords();
        float filter_threahold = options_->get<float>("filter_corpus");
        if(batch_words > max_words * filter_threahold
           || options_->has("inference")) {  // default = 0.85
          // passed filtering
          total_batches_runner++;
          tempBatches.push_back(batch);
        }
        // prepare for next batch
        batchVector.clear();
        currentWords = 0;
        lengths.assign(sets, 0);
        if(stats_)
          cachedStatsIter = stats_->begin();
      }
      // refresh competence_runner with update_cycle
      compentence_runner_ = total_batches_runner / options_->get<int>("update_cycle");
    }

    // turn rest into batch
    // if the batch is not fill up, abadon the rest of the sents
    // unless tempBatches is empty
    if(!batchVector.empty() && options_->has("inference")) {
      tempBatches.push_back(data_->toBatch(batchVector));
    }
    // Shuffle the batches
    if(shuffle_) {
      std::shuffle(tempBatches.begin(), tempBatches.end(), eng_);
    }

    // LOG
    std::deque<BatchPtr> realBatches;
    if(!options_->has("inference") && competence_) {
      int total_batch_size = 0;
      size_t runner = 0;
      if(current_c != 1) {
        while(!tempBatches.empty()) {
          BatchPtr batch = tempBatches.front();
          tempBatches.pop_front();
          total_batch_size += batch->size();
          realBatches.push_back(batch);
          runner++;
        }
      } else
        realBatches = tempBatches;

      //size_t avg = realBatches.size() ? total_batch_size / realBatches.size() : 0;
      // Get the First&Sec sent from first batch
      if (!realBatches.empty()) {
        LOG(info,
            "[CL] T : {}, T_Batches : {}, Compentence : {}, Skipped sents : {}",
            compentence_runner_,
            total_batches_runner,
            current_c,
            skipped_);
        if (current_c == 1. && first_CL_completed){
          first_CL_completed = false;
          LOG(info, "[CL] CL completed, step : {}", compentence_runner_);
        }
      }
      // output all batch's size
      // LOG(info,
      //    "[CL] all batch sents size: {}, avg batch_size:{}, num of batch:{}",
      //    total_batch_size,
      //    avg,
      //    realBatches.size());
    } else
      realBatches = tempBatches;

    if(!realBatches.empty() && !options_->has("inference")) {
      BatchPtr batch;
      for(int j = 0; j < realBatches.size(); ++j) {
        batch = realBatches[j];
        LOG(info,
            "[data] TOP{}, batch, src_len, tgt_len[{}, ({} ,{})] = words[{}, {}]",
            j,
            batch->size(),
            batch->front()->batchWidth(),
            batch->back()->batchWidth(),
            batch->front()->batchWords(),
            batch->back()->batchWords());
        if(j > 4)
          break;
      }
      LOG(info, "[data] Total Batches:{}", realBatches.size());
    }

    //if (realBatches.empty())
    //  LOG(info, "[data] Retrieved zero batch.");
    return realBatches;
  }


  // this starts fillBatches() as a background operation
  void fetchBatchesAsync() {
    ABORT_IF(futureBufferedBatches_.valid(),
             "attempted to restart futureBufferedBatches_ while still running");
    if(!options_->has("inference") && competence_) {
      futureBufferedBatches2_ = threadPool_.enqueue([this]() { return fetchBatches_CL2(0); });
      futureBufferedBatches_ = threadPool_.enqueue([this]() { return fetchBatches_CL2(1); });
      //} else if(!options_->has("inference") && options_->has("gap-training"))
      //  futureBufferedBatches_ = threadPool_.enqueue([this]() { return fetchBatches_GAP(); });
    }
    else
      // in order display batch-info
      futureBufferedBatches_ = threadPool_.enqueue([this]() { return fetchBatches_CL2(0); });
    // futureBufferedBatches_ = threadPool_.enqueue([this]() { return fetchBatches(); });
  }

  //std::deque<BatchPtr> fetchBatchesSync() { return fetchBatches_GAP(); }

  BatchPtr next() {
    if(bufferedBatches_.empty()) {
      // out of data: need to get next batch from background thread
      // We only get here if the future has been scheduled to run; it must be valid.
      // ABORT_IF(!futureBufferedBatches_.valid(),
      //         "attempted to wait for futureBufferedBatches_ when none pending");
      bufferedBatches_ = std::move(futureBufferedBatches_.get());
      // 2 threads
      if(!options_->has("inference") && competence_) {
        std::deque<BatchPtr> buffered2 = std::move(futureBufferedBatches2_.get());
        for(auto& batch : buffered2)
          bufferedBatches_.emplace_back(batch);
      }
      // bufferedBatches_ = fetchBatchesSync();
      // if bg thread returns an empty swath, we hit the end of the epoch
      if(bufferedBatches_.empty()) {
        // increase epoch by 1
        epoch_++;
        return nullptr;
      }
      // and kick off the next bg operation
      fetchBatchesAsync();
    }
    auto batch = bufferedBatches_.front();
    bufferedBatches_.pop_front();
    return batch;
  }

public:
  BatchGenerator(Ptr<DataSet> data,
                 Ptr<Options> options,
                 Ptr<BatchStats> stats = nullptr,
                 const std::vector<Ptr<ExpressionGraph>>& graphs = {})
      : data_(data), options_(options), stats_(stats), threadPool_(4) {

    gap_training_ = NewModTraining(options_, data_->getVocabs(), graphs);
    if(!options_->has("inference")) {
      if(options_->has("sr-freq-file")
         && !options_->get<std::vector<std::string>>("sr-freq-file").empty()) {
        competence_ = New<Competence>(options_, data_->getVocabs()[0]);
        LOG(info, "[data] Running CL special fetching, we have 2 threads are running.");
        options_->set("mod_init_value", competence_->init_value);
        // init competence
      }
      if(options_->has("gap-training")) {
        gap_training_ = NewGapTraining(options_, data_->getVocabs(), graphs);
        IsAsync_ = false;
      }
    }
  }

  ~BatchGenerator() {
    if(futureBufferedBatches_.valid())  // bg thread holds a reference to 'this',
      futureBufferedBatches_.get();     // so must wait for it to complete
  }

  iterator begin() { return iterator(this, next()); }

  iterator end() { return iterator(this, nullptr); }

  // @TODO: get rid of this function, begin() or constructor should figure this out
  void prepare(bool shuffle = true) {
    if(shuffle)
      data_->shuffle();
    else
      data_->reset();
    newlyPrepared_ = true;

    // @TODO: solve this better, maybe use options
    shuffle_ = shuffle;

    // start the background pre-fetch operation
    fetchBatchesAsync();
  }

  // Used to restore the state of a BatchGenerator after
  // an interrupted and resumed training.
  bool restore(Ptr<TrainingState> state, bool shuffle) {
    if(state->epochs == 1 && state->batchesEpoch == 0)
      return false;

    // restore C runner
    compentence_runner_ = state->batches;
    total_batches_runner = compentence_runner_ * options_->get<int>("update_cycle");
    LOG(info, "[CL] C:{}", compentence_runner_);

    LOG(info,
        "[data] Restoring the corpus state to epoch {}, batch {}",
        state->epochs,
        state->batches);

    // remark the current epochs
    epoch_ = state->epochs;

    if(state->epochs > 1) {
      data_->restore(state);
      setRNGState(state->seedBatch);
    }

    prepare(shuffle);
    for(size_t i = 0; i < state->batchesEpoch; ++i)
      next();

    return true;
  }
};

class CorpusBatchGenerator : public BatchGenerator<CorpusBase>, public TrainingObserver {
public:
  CorpusBatchGenerator(Ptr<CorpusBase> data,
                       Ptr<Options> options,
                       Ptr<BatchStats> stats = nullptr,
                       const std::vector<Ptr<ExpressionGraph>>& graphs = {})
      : BatchGenerator(data, options, stats, graphs) {}

  void actAfterEpoch(TrainingState& state) override {
    state.seedBatch = getRNGState();
    state.seedCorpus = data_->getRNGState();
  }
};
}  // namespace data
}  // namespace marian
